{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ9hJS9QJ6B6",
        "outputId": "0fcd9518-1298-463b-a062-dd365d34f2f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: monai in /usr/local/lib/python3.9/dist-packages (1.1.0)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.9/dist-packages (from monai) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from monai) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8->monai) (4.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install monai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UM_YdKoOMJED",
        "outputId": "71cf8bac-d338-4911-c81b-2941211d8023"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Optional, Tuple, Type, Union\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "tjOr2fbPKC6Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from monai.networks.blocks.convolutions import Convolution\n",
        "from monai.networks.layers.factories import Act, Conv, Dropout, Norm, split_args"
      ],
      "metadata": {
        "id": "JE9cYur4KKEw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZu_VrVVMXHo",
        "outputId": "079e7e18-0955-4ca8-95be-deb14bc29b09"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "__all__ = [\"VNet\"]"
      ],
      "metadata": {
        "id": "ntJeEgVYKPbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_acti_layer(act: Union[Tuple[str, Dict], str], nchan: int = 0):\n",
        "    if act == \"prelu\":\n",
        "        act = (\"prelu\", {\"num_parameters\": nchan})\n",
        "    act_name, act_args = split_args(act)\n",
        "    act_type = Act[act_name]\n",
        "    return act_type(**act_args)\n",
        "\n",
        "\n",
        "class LUConv(nn.Module):\n",
        "    def __init__(self, spatial_dims: int, nchan: int, act: Union[Tuple[str, Dict], str], bias: bool = False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.act_function = get_acti_layer(act, nchan)\n",
        "        self.conv_block = Convolution(\n",
        "            spatial_dims=spatial_dims,\n",
        "            in_channels=nchan,\n",
        "            out_channels=nchan,\n",
        "            kernel_size=5,\n",
        "            act=None,\n",
        "            norm=Norm.BATCH,\n",
        "            bias=bias,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv_block(x)\n",
        "        out = self.act_function(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def _make_nconv(spatial_dims: int, nchan: int, depth: int, act: Union[Tuple[str, Dict], str], bias: bool = False):\n",
        "    layers = []\n",
        "    for _ in range(depth):\n",
        "        layers.append(LUConv(spatial_dims, nchan, act, bias))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class InputTransition(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        spatial_dims: int,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        act: Union[Tuple[str, Dict], str],\n",
        "        bias: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # if out_channels % in_channels != 0:\n",
        "        #     raise ValueError(\n",
        "        #         f\"out channels should be divisible by in_channels. Got in_channels={in_channels}, out_channels={out_channels}.\"\n",
        "        #     )\n",
        "\n",
        "        self.spatial_dims = spatial_dims\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.act_function = get_acti_layer(act, out_channels)\n",
        "        self.conv_block = Convolution(\n",
        "            spatial_dims=spatial_dims,\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=5,\n",
        "            act=None,\n",
        "            norm=Norm.BATCH,\n",
        "            bias=bias,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(x.shape)\n",
        "        out = self.conv_block(x)\n",
        "        print(\"intrans\")\n",
        "        print(out.shape)\n",
        "        repeat_num = self.out_channels // self.in_channels\n",
        "        x16 = x.repeat([1, repeat_num, 1, 1, 1][: self.spatial_dims + 2])\n",
        "        print(x16.shape)\n",
        "        #out = self.act_function(torch.add(out, x16))\n",
        "        out = self.act_function(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DownTransition(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        spatial_dims: int,\n",
        "        in_channels: int,\n",
        "        nconvs: int,\n",
        "        act: Union[Tuple[str, Dict], str],\n",
        "        dropout_prob: Optional[float] = None,\n",
        "        dropout_dim: int = 3,\n",
        "        bias: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        conv_type: Type[Union[nn.Conv2d, nn.Conv3d]] = Conv[Conv.CONV, spatial_dims]\n",
        "        norm_type: Type[Union[nn.BatchNorm2d, nn.BatchNorm3d]] = Norm[Norm.BATCH, spatial_dims]\n",
        "        dropout_type: Type[Union[nn.Dropout, nn.Dropout2d, nn.Dropout3d]] = Dropout[Dropout.DROPOUT, dropout_dim]\n",
        "\n",
        "        out_channels = 2 * in_channels\n",
        "        self.down_conv = conv_type(in_channels, out_channels, kernel_size=2, stride=2, bias=bias)\n",
        "        self.bn1 = norm_type(out_channels)\n",
        "        self.act_function1 = get_acti_layer(act, out_channels)\n",
        "        self.act_function2 = get_acti_layer(act, out_channels)\n",
        "        self.ops = _make_nconv(spatial_dims, out_channels, nconvs, act, bias)\n",
        "        self.dropout = dropout_type(dropout_prob) if dropout_prob is not None else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        down = self.act_function1(self.bn1(self.down_conv(x)))\n",
        "        if self.dropout is not None:\n",
        "            out = self.dropout(down)\n",
        "        else:\n",
        "            out = down\n",
        "        out = self.ops(out)\n",
        "        out = self.act_function2(torch.add(out, down))\n",
        "        return out\n",
        "\n",
        "class UpTransition(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        spatial_dims: int,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        nconvs: int,\n",
        "        act: Union[Tuple[str, Dict], str],\n",
        "        dropout_prob: Optional[float] = None,\n",
        "        dropout_dim: int = 3,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        conv_trans_type: Type[Union[nn.ConvTranspose2d, nn.ConvTranspose3d]] = Conv[Conv.CONVTRANS, spatial_dims]\n",
        "        norm_type: Type[Union[nn.BatchNorm2d, nn.BatchNorm3d]] = Norm[Norm.BATCH, spatial_dims]\n",
        "        dropout_type: Type[Union[nn.Dropout, nn.Dropout2d, nn.Dropout3d]] = Dropout[Dropout.DROPOUT, dropout_dim]\n",
        "\n",
        "        self.up_conv = conv_trans_type(in_channels, out_channels // 2, kernel_size=2, stride=2)\n",
        "        self.bn1 = norm_type(out_channels // 2)\n",
        "        self.dropout = dropout_type(dropout_prob) if dropout_prob is not None else None\n",
        "        self.dropout2 = dropout_type(0.5)\n",
        "        self.act_function1 = get_acti_layer(act, out_channels // 2)\n",
        "        self.act_function2 = get_acti_layer(act, out_channels)\n",
        "        self.ops = _make_nconv(spatial_dims, out_channels, nconvs, act)\n",
        "\n",
        "    def forward(self, x, skipx):\n",
        "        if self.dropout is not None:\n",
        "            out = self.dropout(x)\n",
        "        else:\n",
        "            out = x\n",
        "        print(\"uptransition\")\n",
        "        print(out.shape) #(1,256,9,15,15)\n",
        "        skipxdo = self.dropout2(skipx)\n",
        "        print(skipxdo.shape) #(1,128,19,30,30)\n",
        "        out = self.up_conv(out)\n",
        "        print(out.shape) #torch.Size([1, 128, 18, 30, 30])\n",
        "        out = self.bn1(out)\n",
        "        print(out.shape) #torch.Size([1, 128, 18, 30, 30])\n",
        "        out = self.act_function1(out)\n",
        "        print(out.shape) #torch.Size([1, 128, 18, 30, 30])\n",
        "        #out = torch.reshape(out, (out.shape[0],out.shape[1],out.shape[2]+1, out.shape[3], out.shape[4]))\n",
        "        if out.shape[1] !=64:\n",
        "            reshape_out = torch.zeros((out.shape[0],out.shape[1],1, out.shape[3], out.shape[4]))\n",
        "            out = torch.cat((out, reshape_out), 2)\n",
        "        print(out.shape)\n",
        "        xcat = torch.cat((out, skipxdo), 1)\n",
        "        out = self.ops(xcat)\n",
        "        out = self.act_function2(torch.add(out, xcat))\n",
        "        return out\n",
        "\n",
        "class OutputTransition(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        spatial_dims: int,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        act: Union[Tuple[str, Dict], str],\n",
        "        bias: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        conv_type: Type[Union[nn.Conv2d, nn.Conv3d]] = Conv[Conv.CONV, spatial_dims]\n",
        "\n",
        "        self.act_function1 = get_acti_layer(act, out_channels)\n",
        "        self.conv_block = Convolution(\n",
        "            spatial_dims=spatial_dims,\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=5,\n",
        "            act=None,\n",
        "            norm=Norm.BATCH,\n",
        "            bias=bias,\n",
        "        )\n",
        "        self.conv2 = conv_type(out_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # convolve 32 down to 4 channels\n",
        "        out = self.conv_block(x)\n",
        "        out = self.act_function1(out)\n",
        "        out = self.conv2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "CD3okD52KSg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        spatial_dims: int = 3,\n",
        "        in_channels: int = 3,\n",
        "        out_channels: int = 4,\n",
        "        act: Union[Tuple[str, Dict], str] = (\"elu\", {\"inplace\": True}),\n",
        "        dropout_prob: float = 0.5,\n",
        "        dropout_dim: int = 3,\n",
        "        bias: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        if spatial_dims not in (2, 3):\n",
        "            raise AssertionError(\"spatial_dims can only be 2 or 3.\")\n",
        "\n",
        "        self.in_tr = InputTransition(spatial_dims, in_channels, 16, act, bias=bias)\n",
        "        self.down_tr32 = DownTransition(spatial_dims, 16, 1, act, bias=bias)\n",
        "        self.down_tr64 = DownTransition(spatial_dims, 32, 2, act, bias=bias)\n",
        "        self.down_tr128 = DownTransition(spatial_dims, 64, 3, act, dropout_prob=dropout_prob, bias=bias)\n",
        "        self.down_tr256 = DownTransition(spatial_dims, 128, 2, act, dropout_prob=dropout_prob, bias=bias)\n",
        "        self.up_tr256 = UpTransition(spatial_dims, 256, 256, 2, act, dropout_prob=dropout_prob)\n",
        "        self.up_tr128 = UpTransition(spatial_dims, 256, 128, 2, act, dropout_prob=dropout_prob)\n",
        "        self.up_tr64 = UpTransition(spatial_dims, 128, 64, 1, act)\n",
        "        self.up_tr32 = UpTransition(spatial_dims, 64, 32, 1, act)\n",
        "        self.out_tr = OutputTransition(spatial_dims, 32, out_channels, act, bias=bias)\n",
        "    def forward(self, x):\n",
        "        depth = x.shape[2]\n",
        "        print(x.shape)\n",
        "        out16 = self.in_tr(x)\n",
        "        print(out16.shape)\n",
        "        out32 = self.down_tr32(out16)\n",
        "        print(out32.shape)\n",
        "        out64 = self.down_tr64(out32)\n",
        "        print(out64.shape)\n",
        "        out128 = self.down_tr128(out64)\n",
        "        print(out128.shape)\n",
        "        out256 = self.down_tr256(out128)\n",
        "        print(out256.shape)\n",
        "        x = self.up_tr256(out256, out128)\n",
        "        x = self.up_tr128(x, out64)\n",
        "        x = self.up_tr64(x, out32)\n",
        "        x = self.up_tr32(x, out16)\n",
        "        x = self.out_tr(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "lzgRCFgkKW2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "batch_size = 10\n",
        "img_shape = (3, 155, 240, 240)\n",
        "img_tensor = torch.rand(batch_size, *img_shape)"
      ],
      "metadata": {
        "id": "59q19FQRKaMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VNet()\n",
        "outputs = model(img_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukA5qLEuLG8q",
        "outputId": "a0072b8f-1342-4a37-a3c2-d8d10561150e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 3, 155, 240, 240])\n",
            "torch.Size([10, 3, 155, 240, 240])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = VNet()\n",
        "outputs = model(img_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-lB4SBwKdCR",
        "outputId": "8b475d3e-2973-4915-e174-871a3b3ebd75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 155, 240, 240])\n",
            "torch.Size([1, 3, 155, 240, 240])\n",
            "intrans\n",
            "torch.Size([1, 16, 155, 240, 240])\n",
            "torch.Size([1, 15, 155, 240, 240])\n",
            "torch.Size([1, 16, 155, 240, 240])\n",
            "torch.Size([1, 32, 77, 120, 120])\n",
            "torch.Size([1, 64, 38, 60, 60])\n",
            "torch.Size([1, 128, 19, 30, 30])\n",
            "torch.Size([1, 256, 9, 15, 15])\n",
            "uptransition\n",
            "torch.Size([1, 256, 9, 15, 15])\n",
            "torch.Size([1, 128, 19, 30, 30])\n",
            "torch.Size([1, 128, 18, 30, 30])\n",
            "torch.Size([1, 128, 18, 30, 30])\n",
            "torch.Size([1, 128, 18, 30, 30])\n",
            "torch.Size([1, 128, 19, 30, 30])\n",
            "uptransition\n",
            "torch.Size([1, 256, 19, 30, 30])\n",
            "torch.Size([1, 64, 38, 60, 60])\n",
            "torch.Size([1, 64, 38, 60, 60])\n",
            "torch.Size([1, 64, 38, 60, 60])\n",
            "torch.Size([1, 64, 38, 60, 60])\n",
            "torch.Size([1, 64, 38, 60, 60])\n",
            "uptransition\n",
            "torch.Size([1, 128, 38, 60, 60])\n",
            "torch.Size([1, 32, 77, 120, 120])\n",
            "torch.Size([1, 32, 76, 120, 120])\n",
            "torch.Size([1, 32, 76, 120, 120])\n",
            "torch.Size([1, 32, 76, 120, 120])\n",
            "torch.Size([1, 32, 77, 120, 120])\n",
            "uptransition\n",
            "torch.Size([1, 64, 77, 120, 120])\n",
            "torch.Size([1, 16, 155, 240, 240])\n",
            "torch.Size([1, 16, 154, 240, 240])\n",
            "torch.Size([1, 16, 154, 240, 240])\n",
            "torch.Size([1, 16, 154, 240, 240])\n",
            "torch.Size([1, 16, 155, 240, 240])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-dPuusuKfnh",
        "outputId": "4a820a63-c93a-463e-fd24-b77d786cabeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 155, 240, 240])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ACi3Tde1U1t-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}